{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading output data from experiment and processing it to analysis and sampling it (to balance data according to parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import scipy as sp\n",
    "import os\n",
    "import json\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence bin size\n",
    "BINSIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larissapspinelli/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>channel</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>clar_link</th>\n",
       "      <th>clarify</th>\n",
       "      <th>date</th>\n",
       "      <th>datePublish</th>\n",
       "      <th>depth</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>...</th>\n",
       "      <th>rec_qt</th>\n",
       "      <th>recommendations</th>\n",
       "      <th>response</th>\n",
       "      <th>search</th>\n",
       "      <th>setting</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>title</th>\n",
       "      <th>verified</th>\n",
       "      <th>views</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>none</td>\n",
       "      <td>NBC News</td>\n",
       "      <td>UCeY0bbntWzzVIaj2z3QigXg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-14</td>\n",
       "      <td>Published on Mar 29, 2018</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['IAvgXmM7Elg']</td>\n",
       "      <td>L</td>\n",
       "      <td>North%20Korea</td>\n",
       "      <td>logged</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TurboTax Free 2019 Commercial \"Spelling Bee\" (...</td>\n",
       "      <td>Verified</td>\n",
       "      <td>193871.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>none</td>\n",
       "      <td>NBC News</td>\n",
       "      <td>UCeY0bbntWzzVIaj2z3QigXg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>Published on Oct 6, 2017</td>\n",
       "      <td>21.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['wqWMrUH5TuU']</td>\n",
       "      <td>L</td>\n",
       "      <td>Las%20Vegas%20shooting</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Remembering The Las Vegas Shooting Victims | N...</td>\n",
       "      <td>Verified</td>\n",
       "      <td>109732.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>none</td>\n",
       "      <td>NBC News</td>\n",
       "      <td>UCeY0bbntWzzVIaj2z3QigXg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>Published on Nov 14, 2018</td>\n",
       "      <td>9.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['eZDfztfau9A']</td>\n",
       "      <td>L</td>\n",
       "      <td>Hurricane%20Harvey</td>\n",
       "      <td>private</td>\n",
       "      <td>978.0</td>\n",
       "      <td>Former First Lady Michelle Obama Describes Lif...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101891.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>none</td>\n",
       "      <td>NBC News</td>\n",
       "      <td>UCeY0bbntWzzVIaj2z3QigXg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>Started streaming 2 hours ago</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['iGn7LTXgcyY']</td>\n",
       "      <td>L</td>\n",
       "      <td>Las%20Vegas%20shooting</td>\n",
       "      <td>private</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Watch Live: FBI, CIA Directors Testify On 'Wor...</td>\n",
       "      <td>Verified</td>\n",
       "      <td>2154.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>none</td>\n",
       "      <td>NBC News</td>\n",
       "      <td>UCeY0bbntWzzVIaj2z3QigXg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>Published on Nov 15, 2016</td>\n",
       "      <td>21.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['5QY5pLQqIYM']</td>\n",
       "      <td>0</td>\n",
       "      <td>DACA</td>\n",
       "      <td>tor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>President Obama To Donald Trump On DACA: Think...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51976.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 category   channel                channel_id clar_link  clarify  \\\n",
       "0         2.0     none  NBC News  UCeY0bbntWzzVIaj2z3QigXg       NaN        1   \n",
       "1         2.0     none  NBC News  UCeY0bbntWzzVIaj2z3QigXg       NaN       -1   \n",
       "2        14.0     none  NBC News  UCeY0bbntWzzVIaj2z3QigXg       NaN       -1   \n",
       "3         3.0     none  NBC News  UCeY0bbntWzzVIaj2z3QigXg       NaN       -1   \n",
       "4         2.0     none  NBC News  UCeY0bbntWzzVIaj2z3QigXg       NaN       -1   \n",
       "\n",
       "         date                    datePublish  depth  dislikes       ...        \\\n",
       "0  2019-04-14      Published on Mar 29, 2018   21.0      77.0       ...         \n",
       "1  2019-02-06       Published on Oct 6, 2017   21.0     121.0       ...         \n",
       "2  2018-12-07      Published on Nov 14, 2018    9.0      94.0       ...         \n",
       "3  2019-01-29  Started streaming 2 hours ago   20.0      -1.0       ...         \n",
       "4  2019-02-25      Published on Nov 15, 2016   21.0     176.0       ...         \n",
       "\n",
       "  rec_qt  recommendations response                  search  setting  \\\n",
       "0    1.0  ['IAvgXmM7Elg']        L           North%20Korea   logged   \n",
       "1    1.0  ['wqWMrUH5TuU']        L  Las%20Vegas%20shooting   normal   \n",
       "2    1.0  ['eZDfztfau9A']        L      Hurricane%20Harvey  private   \n",
       "3    1.0  ['iGn7LTXgcyY']        L  Las%20Vegas%20shooting  private   \n",
       "4    1.0  ['5QY5pLQqIYM']        0                    DACA      tor   \n",
       "\n",
       "  subscribers                                              title  verified  \\\n",
       "0         1.0  TurboTax Free 2019 Commercial \"Spelling Bee\" (...  Verified   \n",
       "1         1.0  Remembering The Las Vegas Shooting Victims | N...  Verified   \n",
       "2       978.0  Former First Lady Michelle Obama Describes Lif...       NaN   \n",
       "3         1.0  Watch Live: FBI, CIA Directors Testify On 'Wor...  Verified   \n",
       "4         1.0  President Obama To Donald Trump On DACA: Think...       NaN   \n",
       "\n",
       "      views  classification  \n",
       "0  193871.0             1.0  \n",
       "1  109732.0             1.0  \n",
       "2  101891.0             1.0  \n",
       "3    2154.0             1.0  \n",
       "4   51976.0             1.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the file with all data output (including short video)\n",
    "directory='data'  \n",
    "filename = 'allDataMay.csv'\n",
    "file = os.path.join(directory, filename)\n",
    "dataRaw = pd.read_csv(file)\n",
    "\n",
    "# Loading the file with all classified channels\n",
    "filename = 'classificationDataMay.csv'\n",
    "file = os.path.join(directory, filename)\n",
    "dataClas = pd.read_csv(file)\n",
    "\n",
    "# Merging data with classification \n",
    "dataYT = pd.merge(dataRaw, dataClas[['channel_id', 'classification']], on=['channel_id'], how='inner')\n",
    "dataYT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# Auxiliary fuction to data cleaning/pre-processing\n",
    "###\n",
    "\n",
    "# converting sequence extracting []\n",
    "def auxNum(strNum):\n",
    "    try:\n",
    "        return int(strNum[1:-1])\n",
    "    except:\n",
    "        return int(0)\n",
    "\n",
    "# converting number with scale symbols    \n",
    "def numConv(number):\n",
    "    try:\n",
    "        if \"K\" in number:\n",
    "            tmp = number.split('K')\n",
    "            return int(float(tmp[0])*1000)\n",
    "        elif \"M\" in number:\n",
    "            tmp = number.split('M')\n",
    "            return int(float(tmp[0])*1000000)\n",
    "        else :\n",
    "            try:\n",
    "                tmp = int(number)\n",
    "                return tmp\n",
    "            except:\n",
    "                return 0\n",
    "    except:\n",
    "        try:\n",
    "            return int(number)\n",
    "        except:\n",
    "                return 0 \n",
    "            \n",
    "# converting time to seconds            \n",
    "def timeConv(number):\n",
    "    try:\n",
    "        if \":\" in number:\n",
    "            #print(number)\n",
    "            tmp = number.split(':')\n",
    "            num = 0\n",
    "            bas = 1\n",
    "            for i in range(len(tmp)):\n",
    "                #print(i)\n",
    "                num = num + int(tmp[len(tmp)-i-1])*bas\n",
    "                bas = bas*60\n",
    "\n",
    "            return int(num)\n",
    "\n",
    "        else :\n",
    "            try:\n",
    "                tmp = int(number)\n",
    "                return tmp\n",
    "            except:\n",
    "                return 0\n",
    "    except:\n",
    "        try:\n",
    "            return int(number)\n",
    "        except:\n",
    "                return 0                \n",
    "\n",
    "# getting the day difference of two dates\n",
    "def daysDiff(date1, date0):\n",
    "    try:\n",
    "        if \"minutes ago\" in date1:\n",
    "            return 1\n",
    "        else :\n",
    "            date0 = parse(date0)\n",
    "            date1 = parse(date1)\n",
    "    \n",
    "        date2 = date1.date()\n",
    "        date00 = date0.date()\n",
    "        return (date00-date2).days+1\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "# division of two values     \n",
    "def divNum(val1, val2):\n",
    "    try:\n",
    "        val1 = int(val1)\n",
    "        val2 = int(val2)\n",
    "        if val1 < 1 or val2 < 1:\n",
    "            return 0\n",
    "        else: \n",
    "            return float(val1/val2)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "# adjusting classification values to [-1, 0, 1] scale  \n",
    "def auxClas(num):\n",
    "    if num <= 2:\n",
    "        return -1\n",
    "    if num > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# Data cleaning\n",
    "###\n",
    "    \n",
    "dataYT['response'] = np.where((dataYT['response']== '0'), 'top movie', dataYT['response'])\n",
    "dataYT['response'] = np.where((dataYT['response']== 'L'), 'bottom movie', dataYT['response'])\n",
    "dataYT['response'] = np.where((dataYT['response']== 'R'), 'random movie', dataYT['response'])\n",
    "dataYT['ord'] = dataYT.apply(lambda x: auxNum(x['order']), axis=1)\n",
    "\n",
    "dataYT['verify'] = np.where(dataYT['verified']==\"Verified\", 1, dataYT['verified'])\n",
    "dataYT['verify'] = np.where(dataYT['verify']==\"Official Artist Channel\", 1, dataYT['verify'])\n",
    "dataYT['verify'] = np.where(dataYT['verify']==1, 1, 0)\n",
    "\n",
    "dataYT['datePublish']  = dataYT.datePublish.astype(str)\n",
    "dataYT['datePublish'] = dataYT.datePublish.replace({\"Published on \": \"\"}, regex=True)\n",
    "dataYT['datePublish'] = dataYT.datePublish.replace({\"Streamed live on \": \"\"}, regex=True)\n",
    "dataYT['datePublish'] = dataYT.datePublish.replace({\"Started streaming \": \"\"}, regex=True)\n",
    "dataYT['datePublish'] = np.where(dataYT['datePublish'].str.contains(\"minutes ago\", case=False, na=False), dataYT['date'], dataYT['datePublish'])\n",
    "dataYT['dateDays'] = dataYT.apply(lambda x: daysDiff(x['datePublish'], x['date']), axis=1)\n",
    "\n",
    "# drop channels not possible to classify and changing classification range\n",
    "dataYT = dataYT[dataYT['classification']!= 0]\n",
    "dataYT['clas'] = dataYT.apply(lambda x: auxClas(x['classification']), axis=1)\n",
    "\n",
    "dataYT = dataYT.replace(to_replace='\\%20', value=' ', regex=True)\n",
    "\n",
    "dataYT.fillna(1)\n",
    "dataYT['likes'] = dataYT.apply(lambda x: numConv(x['likes']), axis=1)\n",
    "dataYT['likes'] = dataYT.likes.astype(int)\n",
    "dataYT['dislikes'] = dataYT.apply(lambda x: numConv(x['dislikes']), axis=1)\n",
    "dataYT['dislikes'] = dataYT.dislikes.astype(int)\n",
    "dataYT['views'] = dataYT.apply(lambda x: numConv(x['views']), axis=1)\n",
    "dataYT['views'] = dataYT.views.astype(int)\n",
    "dataYT['subscribers'] = dataYT.apply(lambda x: numConv(x['subscribers']), axis=1)\n",
    "dataYT['subscribers'] = dataYT.subscribers.astype(int)\n",
    "dataYT['lenght'] = dataYT.apply(lambda x: timeConv(x['lenght']), axis=1)\n",
    "dataYT['lenght'] = dataYT.lenght.astype(int)\n",
    "\n",
    "# Relative metrics \n",
    "dataYT['handsQt'] = dataYT['likes'] + dataYT['dislikes']\n",
    "dataYT['likeRatio'] = dataYT.apply(lambda x: divNum(x['likes'], x['handsQt']), axis=1)\n",
    "dataYT['dislikeRatio'] = dataYT.apply(lambda x: divNum(x['dislikes'], x['handsQt']), axis=1)\n",
    "dataYT['dislikePerView'] = dataYT.apply(lambda x: divNum(x['dislikes'], x['views']), axis=1)\n",
    "dataYT['likePerView'] = dataYT.apply(lambda x: divNum(x['likes'], x['views']), axis=1)\n",
    "dataYT['handsPerView'] = dataYT.apply(lambda x: divNum(x['handsQt'], x['views']), axis=1)\n",
    "dataYT['viewsPerDay'] = dataYT.apply(lambda x: divNum(x['views'], x['dateDays']), axis=1)\n",
    "dataYT['handsPerDay'] = dataYT.apply(lambda x: divNum(x['handsQt'], x['dateDays']), axis=1)\n",
    "dataYT['viewsPerSubscribers'] = dataYT.apply(lambda x: divNum(x['views'], x['subscribers']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting dataset by singular run/simulation\n",
    "dataYT = dataYT.sort_values(by=['search', 'setting', 'preaction', 'response','date','filename', 'ord'])\n",
    "dataYT.reset_index(inplace=True)\n",
    "\n",
    "# creating index column\n",
    "dataYT['ind'] = dataYT.index\n",
    "\n",
    "# getting sequence using index \n",
    "minInd = dataYT.groupby(['search', 'setting', 'preaction', 'response','date','filename'])['ind'].min()\n",
    "minInd = pd.DataFrame(minInd)\n",
    "minInd.reset_index(inplace=True)\n",
    "minInd.columns = ['search', 'setting', 'preaction', 'response','date','filename', 'minind']\n",
    "dataYT = pd.merge(dataYT, minInd, on=['search', 'setting', 'preaction', 'response','date','filename'], how='left')\n",
    "dataYT['seq'] = dataYT['ind']-dataYT['minind']+1\n",
    "dataYT = dataYT.drop(columns=['minind'])\n",
    "\n",
    "# computing size of the sequences\n",
    "maxInd = dataYT.groupby(['search', 'setting', 'preaction', 'response','date','filename'])['seq'].max()\n",
    "maxInd = pd.DataFrame(maxInd)\n",
    "maxInd.reset_index(inplace=True)\n",
    "maxInd.columns = ['search', 'setting', 'preaction', 'response','date','filename', 'seq_max']\n",
    "dataYT = pd.merge(dataYT, maxInd, on=['search', 'setting', 'preaction', 'response','date','filename'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting sequences by lenght (here only sequences of lenght 20)\n",
    "dataYT = dataYT[dataYT['seq_max'] > 19]\n",
    "dataYT = dataYT[dataYT['seq'] < 21]\n",
    "\n",
    "# Selecting just relevant executions/simulations\n",
    "dataYT = dataYT[(dataYT.response == 'top movie') | ((dataYT.response == 'bottom movie'))]\n",
    "dataYT = dataYT[(dataYT.preaction == \"browsing news\") | (dataYT.preaction == \"none\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins for the movie sequence\n",
    "dataYT['bin'] = dataYT.apply(lambda x: int(int(x['seq']-1)/BINSIZE)+1, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating time division after and before YouTube's policy change\n",
    "dataYT['date'] = pd.to_datetime(dataYT['date'])\n",
    "dataYT['pol'] = np.where(dataYT['date'] < pd.to_datetime(\"2019-02-1\"), 'before', 'after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Unnamed: 0', 'category', 'channel', 'channel_id', 'clar_link',\n",
       "       'clarify', 'date', 'datePublish', 'depth', 'dislikes', 'filename', 'id',\n",
       "       'id,title,channel,channel_id,classification', 'key', 'lenght',\n",
       "       'license', 'likes', 'order', 'preaction', 'rec_qt', 'recommendations',\n",
       "       'response', 'search', 'setting', 'subscribers', 'title', 'verified',\n",
       "       'views', 'classification', 'ord', 'verify', 'dateDays', 'clas',\n",
       "       'handsQt', 'likeRatio', 'dislikeRatio', 'dislikePerView', 'likePerView',\n",
       "       'handsPerView', 'viewsPerDay', 'handsPerDay', 'viewsPerSubscribers',\n",
       "       'ind', 'seq', 'seq_max', 'bin', 'pol'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing all data columns\n",
    "dataYT.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bin</th>\n",
       "      <th>search</th>\n",
       "      <th>setting</th>\n",
       "      <th>preaction</th>\n",
       "      <th>response</th>\n",
       "      <th>date</th>\n",
       "      <th>filename</th>\n",
       "      <th>pol</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April the Giraffe</td>\n",
       "      <td>logged</td>\n",
       "      <td>browsing news</td>\n",
       "      <td>bottom movie</td>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>data-browListNlL-April the Giraffe_2018-10-27_...</td>\n",
       "      <td>before</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April the Giraffe</td>\n",
       "      <td>logged</td>\n",
       "      <td>browsing news</td>\n",
       "      <td>bottom movie</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>data-browListNlL-April the Giraffe_2018-10-30_...</td>\n",
       "      <td>before</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April the Giraffe</td>\n",
       "      <td>logged</td>\n",
       "      <td>browsing news</td>\n",
       "      <td>bottom movie</td>\n",
       "      <td>2018-11-09</td>\n",
       "      <td>data-browListNlL-April the Giraffe_2018-11-09_...</td>\n",
       "      <td>before</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April the Giraffe</td>\n",
       "      <td>logged</td>\n",
       "      <td>browsing news</td>\n",
       "      <td>bottom movie</td>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>data-11browListNlL-April the Giraffe_2018-12-2...</td>\n",
       "      <td>before</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April the Giraffe</td>\n",
       "      <td>logged</td>\n",
       "      <td>browsing news</td>\n",
       "      <td>bottom movie</td>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>data-0browListNlL-April the Giraffe_2019-01-30...</td>\n",
       "      <td>before</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "bin             search setting      preaction      response       date  \\\n",
       "0    April the Giraffe  logged  browsing news  bottom movie 2018-10-27   \n",
       "1    April the Giraffe  logged  browsing news  bottom movie 2018-10-30   \n",
       "2    April the Giraffe  logged  browsing news  bottom movie 2018-11-09   \n",
       "3    April the Giraffe  logged  browsing news  bottom movie 2018-12-22   \n",
       "4    April the Giraffe  logged  browsing news  bottom movie 2019-01-30   \n",
       "\n",
       "bin                                           filename     pol    1    2    3  \\\n",
       "0    data-browListNlL-April the Giraffe_2018-10-27_...  before -0.5 -0.5 -1.0   \n",
       "1    data-browListNlL-April the Giraffe_2018-10-30_...  before -0.5 -1.0 -1.0   \n",
       "2    data-browListNlL-April the Giraffe_2018-11-09_...  before  0.0  0.0 -0.5   \n",
       "3    data-11browListNlL-April the Giraffe_2018-12-2...  before -0.5 -0.5 -0.5   \n",
       "4    data-0browListNlL-April the Giraffe_2019-01-30...  before  0.0 -0.5  0.0   \n",
       "\n",
       "bin    4    5    6    7    8    9   10  \n",
       "0   -1.0 -1.0 -1.0 -1.0  0.0  0.5  0.0  \n",
       "1   -0.5 -0.5 -0.5  0.5  0.0  0.0  0.5  \n",
       "2    0.0  0.0 -0.5 -1.0 -1.0  0.0  0.0  \n",
       "3   -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "4   -0.5  0.0  0.0 -0.5 -0.5  0.0  0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting data sequence average per bin\n",
    "dataSeq = dataYT.groupby(['search', 'setting', 'preaction', 'response','date', 'filename', 'pol', 'bin'])['clas'].mean().unstack().fillna(0)\n",
    "dataSeq = pd.DataFrame(dataSeq)\n",
    "dataSeq.reset_index(inplace=True)\n",
    "dataSeq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April the Giraffe 17\n",
      "Bitcoin Price 20\n",
      "DACA 18\n",
      "Hurricane Harvey 33\n",
      "Hurricane Irma 38\n",
      "Hurricane Jose 24\n",
      "Hurricane Maria 19\n",
      "Las Vegas shooting 44\n",
      "North Korea 24\n",
      "Solar Eclipse 19\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Sampling for each query: 'setting', 'response'\n",
    "# Getting minimum number of sequences per setting to giving query to sample\n",
    "###\n",
    "\n",
    "total = 0\n",
    "for key, group in dataSeq.groupby('search'):\n",
    "    \n",
    "    # Getting mininum value to a giving query\n",
    "    MINV = group.groupby(['setting', 'response']).size().min()\n",
    "    print(key + \" \" + str(MINV))\n",
    "    \n",
    "    # creating group ids\n",
    "    groupid = dataSeq.groupby(['setting',  'response']).size()\n",
    "    groupid = pd.DataFrame(groupid)\n",
    "    groupid.reset_index(inplace=True) \n",
    "    groupid.columns = ['setting',  'response', 'size']\n",
    "    groupid['groupid'] = groupid.index\n",
    "    groupid\n",
    "    \n",
    "    # Merging groupid with dataseq \n",
    "    dataSeqGroup = pd.merge(group, groupid, on=['setting',  'response'], how='left')\n",
    "    \n",
    "    # sampling \n",
    "    dataSizeGroup = dataSeqGroup.groupby('groupid').apply(lambda x: x.sample(MINV)).reset_index(drop=True)\n",
    "    dataSizeGroup = dataSizeGroup.drop(columns=['size', 'groupid'])\n",
    "    if total == 0:\n",
    "            dataSameSizeGroup = dataSizeGroup.copy()\n",
    "            total += MINV\n",
    "    else:\n",
    "            dataSameSizeGroup = dataSameSizeGroup.append(dataSizeGroup)\n",
    "            total += MINV\n",
    "\n",
    "# Merging with initialy parsed data and saving to file\n",
    "dataYT_Sample1 = pd.merge(dataYT, dataSameSizeGroup[['search', 'setting', 'response','date', 'filename']], on=['search', 'setting', 'response','date', 'filename'], how='inner')\n",
    "#dataYT_Sample1.to_csv('data/dataSampledQuerySetRes.csv', index=False)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataYT_Sample01 = dataYT_Sample1[dataYT_Sample1.response == 'top movie']\n",
    "#dataYT_Sample01.to_csv('data/dataSampledQuerySetTop.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April the Giraffe 3\n",
      "Bitcoin Price 7\n",
      "DACA 9\n",
      "Hurricane Harvey 9\n",
      "Hurricane Irma 9\n",
      "Hurricane Jose 7\n",
      "Hurricane Maria 9\n",
      "Las Vegas shooting 14\n",
      "North Korea 7\n",
      "Solar Eclipse 7\n",
      "25920\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Sampling for each query: 'setting', 'response', 'pol' (YouTube policy)\n",
    "# Getting minimum number of sequences per setting to giving query to sample\n",
    "###\n",
    "\n",
    "total = 0\n",
    "for key, group in dataSeq.groupby('search'):\n",
    "    \n",
    "    # Getting mininum value to a giving query\n",
    "    MINV = group.groupby(['setting', 'response', 'pol']).size().min()\n",
    "    print(key + \" \" + str(MINV))\n",
    "    \n",
    "    # creating group ids\n",
    "    groupid = dataSeq.groupby(['setting',  'response', 'pol']).size()\n",
    "    groupid = pd.DataFrame(groupid)\n",
    "    groupid.reset_index(inplace=True) \n",
    "    groupid.columns = ['setting',  'response', 'pol', 'size']\n",
    "    groupid['groupid'] = groupid.index\n",
    "    groupid\n",
    "    \n",
    "    # Merging groupid with dataseq \n",
    "    dataSeqGroup = pd.merge(group, groupid, on=['setting',  'response', 'pol'], how='left')\n",
    "    \n",
    "    # sampling \n",
    "    dataSizeGroup = dataSeqGroup.groupby('groupid').apply(lambda x: x.sample(MINV)).reset_index(drop=True)\n",
    "    dataSizeGroup = dataSizeGroup.drop(columns=['size', 'groupid'])\n",
    "    if total == 0:\n",
    "            dataSameSizeGroup = dataSizeGroup.copy()\n",
    "            total += MINV\n",
    "    else:\n",
    "            dataSameSizeGroup = dataSameSizeGroup.append(dataSizeGroup)\n",
    "            total += MINV\n",
    "\n",
    "# Merging with initialy parsed data and saving to file\n",
    "dataYT_Sample2 = pd.merge(dataYT, dataSameSizeGroup[['search', 'setting', 'response','date', 'filename', 'pol']], on=['search', 'setting', 'response','date', 'filename', 'pol'], how='inner')\n",
    "#dataYT_Sample2.to_csv('data/dataSampledQuerySetResPol.csv', index=False)\n",
    "print(len(dataYT_Sample2.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
